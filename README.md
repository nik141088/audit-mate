1. `clean_data.R` reads csv files present in `processed` folder, cleans them, interpret error columns, and saves final dataset in `tmp` folder. It saves `tmp_train.csv` for training, and `tmp_test.csv` for testing. The test/train splits are mindful of categorical variables having low span. We only keep those columns where all categorical combinations are present in both test and train datasets.
2. `h2o_in_R.R` reads the test and train datasets and performs model training. I use Gradient Boosted Machine (GBM) with balanced classes as the algorithm. The trained model is stored for prediction later. There is also some code depicting confusion matrices, hyper-parameter serach etc.
3. `h2o_plumber.R` reads the saved model file and listens for API calls for predictions. These API calls can be made from any platform: excel, web, python etc.
